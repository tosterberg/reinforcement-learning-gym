{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "    In this weeks experiment we will be using a grid world with a goal of finding a policy that given any starting position will lead to a positive reward exit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment\n",
    "Words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "Words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Appendix\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_large_grid' from 'reinforce.utils.rlgridworld.standard_grid' (C:\\Users\\Tyler\\reinforcement-learning-gym\\reinforce\\utils\\rlgridworld\\standard_grid.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[1;32mIn [77]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mreinforce\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrlgridworld\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstandard_grid\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m create_standard_grid, create_negative_grid, create_large_grid \n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mreinforce\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrlgridworld\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01malgorithms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m iterative_policy_evaluation, compute_policy_from_values\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'create_large_grid' from 'reinforce.utils.rlgridworld.standard_grid' (C:\\Users\\Tyler\\reinforcement-learning-gym\\reinforce\\utils\\rlgridworld\\standard_grid.py)"
     ]
    }
   ],
   "source": [
    "from reinforce.utils.rlgridworld.standard_grid import create_standard_grid, create_negative_grid, create_large_grid\n",
    "from reinforce.utils.rlgridworld.algorithms import iterative_policy_evaluation, compute_policy_from_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Value Iteration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from page 83 of Sutton and Barto, RL 2nd. Ed.\n",
    "def value_iteration(gw, gamma=0.9, epsilon=0.001):\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        biggest_change_in_value = 0\n",
    "        for node in gw:\n",
    "            state = node.state\n",
    "            if not gw.is_terminal(state) and not gw.is_barrier(state):\n",
    "                old_value = gw.get_value(state)\n",
    "                new_value = float('-inf')\n",
    "                # valid decisions and rewards at current state\n",
    "                dr = gw.valid_decisions_and_rewards(state)\n",
    "                for action, reward in dr.items():\n",
    "                    reward = gw.get_reward_for_action(state, action)\n",
    "                    value_at_dest = gw.get_value_at_destination(state, action)\n",
    "                    value = reward + gamma*value_at_dest\n",
    "                    if value > new_value:\n",
    "                        new_value = value\n",
    "                    gw.set_value(state, new_value)\n",
    "                biggest_change_in_value = max(biggest_change_in_value,\n",
    "                                                  abs(new_value - old_value))\n",
    "        if biggest_change_in_value < epsilon:\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gw = create_standard_grid()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Initial Values\")\n",
    "gw.print_values()\n",
    "\n",
    "# compute values\n",
    "value_iteration(gw)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Values after Value Iteration\")\n",
    "gw.print_values()\n",
    "\n",
    "# compute policy from values\n",
    "policy = compute_policy_from_values(gw)\n",
    "\n",
    "print(\"\")\n",
    "print(\"New Policy\")\n",
    "gw.print_policy(policy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gw = create_negative_grid()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Initial Values\")\n",
    "gw.print_values()\n",
    "\n",
    "# compute values\n",
    "value_iteration(gw)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Values after Value Iteration\")\n",
    "gw.print_values()\n",
    "\n",
    "# compute policy from values\n",
    "policy = compute_policy_from_values(gw)\n",
    "\n",
    "print(\"\")\n",
    "print(\"New Policy\")\n",
    "gw.print_policy(policy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Policy Iteration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gw = create_standard_grid()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "policy = {\n",
    "    (0,0):'right', (0,1):'right',(0,2):'right',(0,3):'up',\n",
    "    (1,0):'up', (1,1):'', (1,2):'right', (1,3):'',\n",
    "    (2,0):'right', (2,1):'right', (2,2):'right', (2,3):''\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from page 80 of Sutton and Barto, RL, 2nd. Ed.\n",
    "def policy_iteration(gw, policy, gamma=0.9, epsilon=0.001):\n",
    "    while True:\n",
    "        # perform iterative policy evaluation to update values\n",
    "        iterative_policy_evaluation(gw, policy, gamma, epsilon)\n",
    "        # update policy from new values\n",
    "        new_policy = compute_policy_from_values(gw, gamma)\n",
    "        # see if policy has changed\n",
    "        for action in policy:\n",
    "            if policy[action] == new_policy[action]:\n",
    "                policy_stable = True\n",
    "            else:\n",
    "                policy_stable = False\n",
    "                break\n",
    "        # update policy\n",
    "        policy = new_policy\n",
    "        # repeat until policy does not change\n",
    "        if policy_stable == True:\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(\"Initial Policy\")\n",
    "gw.print_policy(policy)\n",
    "print(\"\")\n",
    "\n",
    "# note: this execution of iterative policy evaluation is not part\n",
    "# of the policy iteration algorithm.  It is for the purpose of\n",
    "# displaying the values associated with the input policy\n",
    "\n",
    "iterative_policy_evaluation(gw, policy)\n",
    "print(\"Initial Policy Values\")\n",
    "gw.print_values()\n",
    "\n",
    "# run policy iteration algorithm\n",
    "policy_iteration(gw, policy)\n",
    "# compute policy from optimal values\n",
    "new_policy = compute_policy_from_values(gw)\n",
    "\n",
    "# print new policy and values\n",
    "print(\"\")\n",
    "print(\"New Policy\")\n",
    "gw.print_policy(new_policy)\n",
    "print(\"\")\n",
    "print(\"New Policy Values\")\n",
    "gw.print_values()\n",
    "print(\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gw = create_negative_grid()\n",
    "\n",
    "policy = {\n",
    "    (0,0):'right', (0,1):'right',(0,2):'right',(0,3):'up',\n",
    "    (1,0):'up', (1,1):'', (1,2):'right', (1,3):'',\n",
    "    (2,0):'right', (2,1):'right', (2,2):'right', (2,3):''\n",
    "    }\n",
    "\n",
    "print(\"\")\n",
    "print(\"Initial Policy\")\n",
    "gw.print_policy(policy)\n",
    "print(\"\")\n",
    "\n",
    "# note: this execution of iterative policy evaluation is not part\n",
    "# of the policy iteration algorithm.  It is for the purpose of\n",
    "# displaying the values associated with the input policy\n",
    "\n",
    "iterative_policy_evaluation(gw, policy)\n",
    "print(\"Initial Policy Values\")\n",
    "gw.print_values()\n",
    "\n",
    "# run policy iteration algorithm\n",
    "policy_iteration(gw, policy)\n",
    "# compute policy from optimal values\n",
    "new_policy = compute_policy_from_values(gw)\n",
    "\n",
    "# print new policy and values\n",
    "print(\"\")\n",
    "print(\"New Policy\")\n",
    "gw.print_policy(new_policy)\n",
    "print(\"\")\n",
    "print(\"New Policy Values\")\n",
    "gw.print_values()\n",
    "print(\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Iterative Policy Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gw = create_standard_grid()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "policy = {\n",
    "    (0,0):'up', (0,1):'right',(0,2):'right',(0,3):'up',\n",
    "    (1,0):'up', (1,1):'', (1,2):'right', (1,3):'',\n",
    "    (2,0):'right', (2,1):'right', (2,2):'right', (2,3):''\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Policy\")\n",
    "gw.print_policy(policy)\n",
    "print(\"Initial Values\")\n",
    "gw.print_values()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(gw, policy, gamma=0.9, theta=0.001):\n",
    "\n",
    "    while True:\n",
    "        biggest_change = 0\n",
    "        for node in gw:\n",
    "            state = node.state\n",
    "            if not gw.is_terminal(state) and not gw.is_barrier(state):\n",
    "                # get current (old) value\n",
    "                old_value = gw.get_value(state)\n",
    "                # get action from policy\n",
    "                action = policy[state]\n",
    "                # get immediate reward for action\n",
    "                reward = gw.get_reward_for_action(state, action)\n",
    "                # get value at destination state\n",
    "                value_at_dest = gw.get_value_at_destination(state, action)\n",
    "                # compute new value\n",
    "                new_value = reward + gamma*value_at_dest\n",
    "                # set new value for state\n",
    "                gw.set_value(state, new_value)\n",
    "                # see if |new_value-old_value| is larger than biggest_change\n",
    "                biggest_change = max(\n",
    "                    biggest_change, abs(new_value-old_value))\n",
    "        # iterated over all states, so see if biggest_change is small enough\n",
    "        if biggest_change < theta:\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Policy\")\n",
    "gw.print_policy(policy)\n",
    "iterative_policy_evaluation(gw, policy, gamma = 0.9)\n",
    "print(\"Values for the policy\")\n",
    "gw.print_values()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gw = create_negative_grid()\n",
    "\n",
    "policy = {\n",
    "    (0,0):'up', (0,1):'right',(0,2):'right',(0,3):'up',\n",
    "    (1,0):'up', (1,1):'', (1,2):'right', (1,3):'',\n",
    "    (2,0):'right', (2,1):'right', (2,2):'right', (2,3):''\n",
    "    }\n",
    "\n",
    "print(\"Policy\")\n",
    "gw.print_policy(policy)\n",
    "iterative_policy_evaluation(gw, policy, gamma = 0.9)\n",
    "print(\"Values for the policy\")\n",
    "gw.print_values()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Policy from Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gw = create_standard_grid()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "policy = {\n",
    "    (0,0):'up', (0,1):'right',(0,2):'right',(0,3):'up',\n",
    "    (1,0):'up', (1,1):'', (1,2):'right', (1,3):'',\n",
    "    (2,0):'right', (2,1):'right', (2,2):'right', (2,3):''\n",
    "    }\n",
    "print(\"Input Policy\")\n",
    "gw.print_policy(policy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iterative_policy_evaluation(gw, policy, gamma = 0.9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Values for the input policy\")\n",
    "gw.print_values()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_policy_from_values(gw, gamma = 0.9):\n",
    "    # create null policy dictionary\n",
    "    policy = {}\n",
    "    # loop over all states\n",
    "    for i in range(gw.M):\n",
    "        for j in range(gw.N):\n",
    "            state = (i,j)\n",
    "            # assign 'no' policy to barrier states, there are no actions at barrier states\n",
    "            if gw.is_barrier(state):\n",
    "                policy[state] = ''\n",
    "            # assign 'no' policy to terminal sttes, there are no actions at terminal states\n",
    "            if gw.is_terminal(state):\n",
    "                policy[state] = ''\n",
    "            # for all non terminal and non barrier states\n",
    "            if not gw.is_terminal(state) and not gw.is_barrier(state):\n",
    "                # set candidate best action and best value\n",
    "                best_action = None\n",
    "                best_value = float('-inf')\n",
    "                # get dictionary of all valid decisions and rewards at current state (i,j)\n",
    "                dr = gw.valid_decisions_and_rewards(state)\n",
    "                # iterate over all action, reward in\n",
    "                for action, reward in dr.items():\n",
    "                    # get reward for current action\n",
    "                    reward = gw.get_reward_for_action(state,action)\n",
    "                    # get the value of the destination state for the current action\n",
    "                    value_at_dest = gw.get_value_at_destination(state,action)\n",
    "                    # compute candidate vale\n",
    "                    value = reward + gamma*value_at_dest\n",
    "                    # if value is better, then update best action and best value\n",
    "                    if value > best_value:\n",
    "                        best_value = value\n",
    "                        best_action = action\n",
    "                # add best action to the policy dictionary\n",
    "                policy[state] = best_action\n",
    "    return policy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_policy = compute_policy_from_values(gw)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Original Policy\")\n",
    "gw.print_policy(policy)\n",
    "print(\"\")\n",
    "print(\"New Policy\")\n",
    "gw.print_policy(new_policy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gw = create_negative_grid()\n",
    "\n",
    "policy = {\n",
    "    (0,0):'up', (0,1):'right',(0,2):'right',(0,3):'up',\n",
    "    (1,0):'up', (1,1):'', (1,2):'right', (1,3):'',\n",
    "    (2,0):'right', (2,1):'right', (2,2):'right', (2,3):''\n",
    "    }\n",
    "print(\"Input Policy\")\n",
    "gw.print_policy(policy)\n",
    "\n",
    "iterative_policy_evaluation(gw, policy, gamma = 0.9)\n",
    "\n",
    "new_policy = compute_policy_from_values(gw)\n",
    "\n",
    "print(\"Original Policy\")\n",
    "gw.print_policy(policy)\n",
    "print(\"\")\n",
    "print(\"New Policy\")\n",
    "gw.print_policy(new_policy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Large Grid Experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gw = create_large_grid()\n",
    "\n",
    "policy = {\n",
    "    (0,0):'up', (0,1):'right',(0,2):'right',(0,3):'up', (0, 4): 'left',\n",
    "    (1,0):'up', (1,1):'', (1,2):'right', (1,3):'down', (1, 4): 'down',\n",
    "    (2,0):'right', (2,1):'right', (2,2):'right', (2,3):'', (2,4): 'down',\n",
    "    (3,0): 'down', (3,1): 'right', (3,2): 'right', (3,3): 'right', (3,4): 'down',\n",
    "    (4,0): 'down', (4,1): 'right', (4,2): 'right', (4,3): '', (4,4): 'left',\n",
    "}\n",
    "print(\"Input Policy\")\n",
    "gw.print_policy(policy)\n",
    "\n",
    "iterative_policy_evaluation(gw, policy, gamma = 0.9)\n",
    "\n",
    "new_policy = compute_policy_from_values(gw)\n",
    "\n",
    "print(\"Original Policy\")\n",
    "gw.print_policy(policy)\n",
    "print(\"\")\n",
    "print(\"New Policy\")\n",
    "gw.print_policy(new_policy)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
