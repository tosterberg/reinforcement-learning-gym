{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;This weeks weeks experiment is meant to begin tracing out our understanding of Markov Decicsion Processes (MDP's) and how dynamic programming can be used to solve these kinds of problems. We will start by setting up an environment that will act as our MDP, discuss the different ways that it could be setup, and then test the policies from last week to see if any of them are effective. Finally, we will discuss what will need to be implemented to better solve these problems and how we can find the optimal solution to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The process for this experiment will be to\n",
    "1. Implement a version of \"GridWorld\", where reward for the agent is based on state and action.\n",
    "2. Test our \"GridWorld\" with the algorithms discussed last week.\n",
    "3. Plot and analyze their performance.\n",
    "4. Discuss what solves the optimal solution, and how we might solve our \"GridWorld\"\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;GridWorld can be set up a few different ways. The way that it is implemented in our example notebook is to have a grid of `x` length and `y` height, creating a number of states that our grid can have of `x * y`. Every state that we have has up to four actions that can be taken in that space. The agent can move up, down, left, or right. I am writing an implementation that is closer to what is found in \"Reinforcement Learning: An Introduction\" by Sutton and Barto. In the one given in the example notebook, actions were limited to allowed actions, i.e. the agent cannot take an action that leaves the board, and \"barrier\" nodes function the same way; where the agent cannot move into this node. The world has two terminal nodes, one positive and one negative. Goal for the agent is to make it to the positive terminal node, going to the negative node gives a punishment, and all other actions are zero. There is one alternative setup for this where all other actions are -0.1 and will discourage unnecessary movement around the world. In the configuration of GridWorld that I initially started with there are instead no barrier nodes, and instead, if the agent takes an illegal action it is punished with negative reward, and there are only a couple nodes on the board that provide reward and the reward is given when leaving that node.\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;I chose to use this setup because it lends itself more optimally to our previous algorithms, and since we are not implementing new dynamic policies that solve GridWorld this week it made the most sense for creating an experiment. My reason being is that our previous algorithms do not have a stored state, and instead only assign expected value to actions. With punishment for falling off of the grid, we can at least see the algorithms attempt to stay on the grid. Our GridWorld is a 4 by 4 area with two reward nodes, one with reward 10 for leaving the square and not falling off of the grid, and one with a reward of 5 for the same. Falling off of the map is -1 reward and results in no state change. Given this configuration, and us using Îµ-greedy, greedy, and random policies that do not yet have state awareness I am expecting our best algorithms to trend towards zero, with worse algorithms being more negative, but that none will be positive reward seekers for this environment.\n",
    "\n",
    "![Graph of the data of the agents running in the GridWorld environment](./Data_GridWorld.png)\n",
    "*Figure. 1: The data of the agents running in the GridWorld environment*\n",
    "\n",
    "![Graph of the average reward of the agents running in the GridWorld environment](./AverageReward_GridWorld.png)\n",
    "*Figure. 2: The average reward of the agents running in the GridWorld environment*\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;It is clear from Figure 1 and Figure 2 that we are not getting any positive reward from our policy algorithms when they are not aware of the state of the environment. In fact the best mean reward per step for an agent was -0.0006 (Optimistic Initial Values Greedy), and the worst was -0.0320 (Random Movement). Now in either setup of GridWorld we can determine an optimal policy for any given board configuration by solving the Bellman optimality equation for the state-value function. This will give us the maximum reward we can achieve from any given state. Additionally, we will need to solve the optimal action-value function for all states, so that we can also know what the optimal action is for any given states. These are trivially calculable for finite MDPs with constant reward functions and small enough state space that every state-action-value configuration can be calculated. Although, this narrows down what it can be applied to, we can use this to compare our agents policy to an optimal policy for small GridWorlds next week.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;This week's experiment has shown where are policies from last week begin to fail in providing positive reward. Going from the actions being based on stationary reward functions, and the state of the environment being irrelevant to the reward of any given action to one where the state clearly determines the reward and our actions influence the state of the environment. It is clear that we need policies that have a concept of state, and being able to estimate a state-value function and action-value function together will inform the policies for our agents in a way that they can maximize reward."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
